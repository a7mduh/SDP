{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import my_tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>language</th>\n",
       "      <th>safety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int _gnutls_ciphertext2compressed(gnutls_sessi...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>static char *make_filename_safe(const char *fi...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unpack_Z_stream(int fd_in, int fd_out)\\n{\\n\\tI...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>static void cirrus_do_copy(CirrusVGAState *s, ...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>glue(cirrus_bitblt_rop_fwd_, ROP_NAME)(CirrusV...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code language      safety\n",
       "0  int _gnutls_ciphertext2compressed(gnutls_sessi...    C/C++  vulnerable\n",
       "1  static char *make_filename_safe(const char *fi...    C/C++  vulnerable\n",
       "2  unpack_Z_stream(int fd_in, int fd_out)\\n{\\n\\tI...    C/C++  vulnerable\n",
       "3  static void cirrus_do_copy(CirrusVGAState *s, ...    C/C++  vulnerable\n",
       "4  glue(cirrus_bitblt_rop_fwd_, ROP_NAME)(CirrusV...    C/C++  vulnerable"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"Sample.csv\")\n",
    "sample = sample[[\"code\", \"language\", \"safety\"]]\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>language</th>\n",
       "      <th>safety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int _gnutls_ciphertext2compressed(gnutls_sessi...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>static char *make_filename_safe(const char *fi...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unpack_Z_stream(int fd_in, int fd_out)\\n{\\n\\tI...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>static void cirrus_do_copy(CirrusVGAState *s, ...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>glue(cirrus_bitblt_rop_fwd_, ROP_NAME)(CirrusV...</td>\n",
       "      <td>C/C++</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code language      safety\n",
       "0  int _gnutls_ciphertext2compressed(gnutls_sessi...    C/C++  vulnerable\n",
       "1  static char *make_filename_safe(const char *fi...    C/C++  vulnerable\n",
       "2  unpack_Z_stream(int fd_in, int fd_out)\\n{\\n\\tI...    C/C++  vulnerable\n",
       "3  static void cirrus_do_copy(CirrusVGAState *s, ...    C/C++  vulnerable\n",
       "4  glue(cirrus_bitblt_rop_fwd_, ROP_NAME)(CirrusV...    C/C++  vulnerable"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sample[:1001]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, temp_data = train_test_split(dataset, test_size=0.2, stratify=dataset['language'], random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['language'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def create_prompt(code, language):\n",
    "      prompt = f'''\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Is this {language} code safe or vulnerable to software vulnerabilities:\n",
    "{code}\n",
    "\n",
    "### Response:\n",
    "'''\n",
    "      return prompt\n",
    "max_input_length = 1500\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        code = self.data['code'].iloc[idx]  # Assuming text is in the first column\n",
    "        language = self.data['language'].iloc[idx]\n",
    "        label = self.data['safety'].replace({'safe': 0, 'vulnerable': 1}).iloc[idx]\n",
    "        prompts = create_prompt(code, language)\n",
    "        return {'prompt': prompts, 'label': label, 'index': idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model_name = \"tiiuae/falcon-rw-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token # setting tokenizer pad token\n",
    "tokenizer.padding_side = 'left' # setting padding to left as decoder models run from left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    indices = [item['index'] for item in batch]\n",
    "    prompts = [item['prompt'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    input_ids = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_input_length).to(device)\n",
    "    labels = torch.tensor(labels).to(device)\n",
    "    return {\"indices\": indices, \"input_ids\": input_ids['input_ids'], 'attention_mask': input_ids['attention_mask'], \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"Sample.csv\")\n",
    "sample = sample[:1000]\n",
    "sample = sample[['code', 'language', 'safety', 'dataset']]\n",
    "dataset = TextDataset(sample, None, None)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6359b5b0ba4d5088c4a1c17e5e157c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18164/1447112091.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  label = self.data['safety'].replace({'safe': 0, 'vulnerable': 1}).iloc[idx]\n",
      "/home/ahmed/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ahmed/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.92` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_18164/1447112091.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  label = self.data['safety'].replace({'safe': 0, 'vulnerable': 1}).iloc[idx]\n",
      "/home/ahmed/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ahmed/miniconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.92` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  counter = 0\n",
    "  for batch in tqdm(data_loader):\n",
    "    indices = batch['indices']\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    outputs = model.generate( # function to generate output completion\n",
    "        input_ids, # passing input prompt\n",
    "        attention_mask=attention_mask, # including attention mask\n",
    "        max_length=3000,  # specifying maximum length of model generation\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=0.8, # hyper-parameter\n",
    "        top_p=0.92, # hyper-parameter\n",
    "        top_k=50, # hyper-parameter\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating the same n-grams so will prevent the same token to be repeated n times\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    ) # generating outputs from inputs\n",
    "    generated_texts = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in outputs]\n",
    "    responses = [text.split(\"### Response:\\n\")[1].strip() if \"### Response:\\n\" in text else text for text in generated_texts]\n",
    "    for idx, response in zip(indices, responses):\n",
    "      sample.at[idx, 'completion'] = response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
